{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (c:\\Users\\user\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:325139)",
      "at w.execute (c:\\Users\\user\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:324460)",
      "at w.start (c:\\Users\\user\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:320276)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at t.CellExecutionQueue.executeQueuedCells (c:\\Users\\user\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:334803)",
      "at t.CellExecutionQueue.start (c:\\Users\\user\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:334343)"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch, numpy as np\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "import tianshou as ts\n",
    "from tianshou.policy.modelfree.ppo import PPOPolicy\n",
    "from tianshou.utils import TensorboardLogger, WandbLogger\n",
    "from agents import TwoAgentPolicy\n",
    "from agents.lib_agents import SinePolicy, GreedyPolicy\n",
    "from agents.lib_agents import DQN, PPO\n",
    "from functools import partial\n",
    "from utils import general_make_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Hyper parameters for the example\n",
    "NUM_TRAIN_ENVS = 3\n",
    "NUM_TEST_ENVS = 5\n",
    "BAR_ACTION_K = 7 # Number of action values to discretize into\n",
    "BUFFER_SIZE = 2000\n",
    "BUFFER_NUM = 10\n",
    "\n",
    "# Parameters for environment\n",
    "train_params = {\n",
    "    'flatten' : {},\n",
    "    # 'discrete': {\n",
    "    #     'k': BAR_ACTION_K,\n",
    "    # }\n",
    "}\n",
    "test_params = {\n",
    "    'flatten' : {},\n",
    "    'render': {\n",
    "        'eps': 0.02\n",
    "    },\n",
    "    # 'discrete': {\n",
    "    #     'k': BAR_ACTION_K,\n",
    "    # }\n",
    "}\n",
    "env = general_make_env(params=train_params)\n",
    "# creating policies\n",
    "p1 = SinePolicy()\n",
    "# p2 = GreedyPolicy(agent='bar', disc_k=7)\n",
    "# p2 = DQN(\n",
    "#     env.observation_space.shape, \n",
    "#     env.action_space['bar'].shape\n",
    "#     )(\n",
    "#         discount_factor=0.99, \n",
    "#         estimation_step=5, \n",
    "#         target_update_freq=320\n",
    "#     )\n",
    "# p2.set_eps(0.1)\n",
    "p2 = PPO(\n",
    "    env.observation_space.shape,\n",
    "    env.action_space['bar']\n",
    "    )(\n",
    "        discount_factor=0.99,\n",
    "    )\n",
    "\n",
    "policy = TwoAgentPolicy(observation_space=env.observation_space, action_space=env.action_space, policies=(p1, p2))\n",
    "\n",
    "\n",
    "# create environment\n",
    "train_envs = ts.env.DummyVectorEnv([partial(general_make_env, params=train_params) for _ in range(NUM_TRAIN_ENVS)])\n",
    "test_envs = ts.env.DummyVectorEnv([partial(general_make_env, params=test_params) for _ in range(NUM_TEST_ENVS)])\n",
    "\n",
    "# setup collector\n",
    "train_collector = ts.data.Collector(\n",
    "    policy, \n",
    "    train_envs, \n",
    "    ts.data.VectorReplayBuffer(BUFFER_SIZE, BUFFER_NUM), exploration_noise=True)\n",
    "test_collector = ts.data.Collector(\n",
    "    policy, \n",
    "    test_envs, exploration_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\tmp/ipykernel_9464/1066587048.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m result = ts.trainer.onpolicy_trainer(\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_collector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_collector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mmax_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tianshou\\trainer\\onpolicy.py\u001b[0m in \u001b[0;36monpolicy_trainer\u001b[1;34m(policy, train_collector, test_collector, max_epoch, step_per_epoch, repeat_per_collect, episode_per_test, batch_size, step_per_collect, episode_per_collect, train_fn, test_fn, stop_fn, save_fn, save_checkpoint_fn, resume_from_log, reward_metric, logger, verbose, test_in_train)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[0mtest_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_stat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[0mtest_in_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_in_train\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtrain_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m     test_result = test_episode(\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_collector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_per_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0menv_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_metric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tianshou\\trainer\\utils.py\u001b[0m in \u001b[0;36mtest_episode\u001b[1;34m(policy, collector, test_fn, epoch, n_episode, logger, global_step, reward_metric)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtest_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mtest_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_episode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_episode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreward_metric\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mrew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"rews\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tianshou\\data\\collector.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self, n_step, n_episode, random, render, no_grad)\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0maction_remap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;31m# step in env\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_remap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mready_env_ids\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m             \u001b[0mobs_next\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tianshou\\env\\venvs.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action, id)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# different len(obs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[0mobs_stack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         rew_stack, done_stack, info_stack = map(\n\u001b[0m\u001b[0;32m    257\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrew_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         )\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all input arrays must have the same shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# logging\n",
    "# First sign in on wandb\n",
    "# logger = WandbLogger(\n",
    "#     save_interval=1,\n",
    "#     project=\"test-project\",\n",
    "#     name='Sarthak Rout',\n",
    "#     entity='penalty-shot-project',\n",
    "# )\n",
    "# training\n",
    "\n",
    "result = ts.trainer.onpolicy_trainer(\n",
    "    policy, train_collector, test_collector,\n",
    "    max_epoch=10, \n",
    "    step_per_epoch=10000, \n",
    "    repeat_per_collect=2,\n",
    "    episode_per_test=100, \n",
    "    batch_size=64,\n",
    "    # update_per_step=0.1, \n",
    "    episode_per_collect=10,    \n",
    "    \n",
    "    #  logger=logger\n",
    "     )\n",
    "np.stack()\n",
    "print(f'Finished training! Use {result}')\n",
    "\n",
    "torch.save(policy.state_dict(), './opt_policy/policy_{:%Y-%m-%d_%H-%M-%S}.pth'.format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f23faf4bfe871c203c8bec80520af5927fc7cb1ae3bd834ddf554ee587ad1c05"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
